#Librerie
import random
import requests
import time
from bs4 import BeautifulSoup
import logging
from bs4 import BeautifulSoup
import pandas as pd
from tqdm import tqdm
from time import sleep
from random import randint
from requests.exceptions import ConnectionError, Timeout, ReadTimeout

# Scarica la lista di User-Agent
response = requests.get('https://raw.githubusercontent.com/AlessandroVaccarino/user-agents/main/agents/desktop.txt')

# Divide il testo in una lista riga per riga
user_agents_list = response.text.splitlines()

# Seleziona casualmente 10 User-Agent e li stampa
for _ in range(10):
    user_agent = random.choice(user_agents_list)
    print(user_agent)
#Funzione per gli URL
# Scarica una lista di User-Agent da GitHub
def get_user_agents():
    try:
        response = requests.get('https://raw.githubusercontent.com/AlessandroVaccarino/user-agents/main/agents/desktop.txt')
        if response.status_code == 200:
            return response.text.splitlines()
        else:
            return ["Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"]
    except requests.RequestException:
        return ["Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"]

# Funzione per estrarre i link delle squadre
def get_squad_urls(url):
    squad_urls = []

    # Seleziona un User-Agent casuale
    user_agents_list = get_user_agents()
    session = requests.Session()
    headers = {
        'User-Agent': random.choice(user_agents_list),
        'Referer': 'https://www.transfermarkt.com/',
        'Accept-Language': 'en-US,en;q=0.9',
    }

    try:
        response = session.get(url, headers=headers, timeout=10)
        #print(f"Status Code: {response.status_code}")

        if response.status_code != 200:
            print("‚ùå Problema con la richiesta! Potresti essere stato bloccato.")
            return squad_urls

        soup = BeautifulSoup(response.content, 'html.parser')

        # Prova a trovare il div con id 'yw1'
        div_yw1 = soup.find('div', {'id': 'yw1'})

        if not div_yw1:
            print("‚ö†Ô∏è 'yw1' non trovato! Provo con un altro div...")
            div_yw1 = soup.find('div', {'class': 'responsive-table'})

        if not div_yw1:
            print("‚ö†Ô∏è Ancora nulla! Il layout potrebbe essere cambiato o sei stato bloccato.")
            return squad_urls

        rows = div_yw1.find_all('tr')
        for row in rows:
            cell = row.find('td')
            if cell:
                link = cell.find('a')
                if link and 'href' in link.attrs:
                    squad_url = "https://www.transfermarkt.com" + link['href']
                    squad_urls.append(squad_url)

        # Ritardo casuale per evitare blocchi
        time.sleep(random.uniform(1, 3))

    except requests.exceptions.RequestException as e:
        print(f"Errore durante la richiesta: {e}")

    return squad_urls

# üîπ ESEMPIO DI UTILIZZO:
url = "https://www.transfermarkt.com/premier-league/startseite/wettbewerb/GB1"  # Cambia con un URL valido
squad_urls = get_squad_urls(url)

# Stampa i link trovati
for squad_url in squad_urls:
    print(squad_url)
#Funzione per il valore
def parse_currency_value(value_string):
    numeric_string = value_string.replace("‚Ç¨", "").replace(",", "").replace(".00", "")
    if numeric_string.endswith("m"):
        numeric_value = float(numeric_string[:-1])  # remove the "m" suffix and convert to int
    elif numeric_string.endswith("k"):
        numeric_value = float(numeric_string[:-1]) / 1000  # remove the "k" suffix and convert to decimal
    else:
        raise ValueError("Invalid currency format")
    return numeric_value
#Estrarre Info per campionato e stagione
# üîπ Imposta stagioni e campionati da analizzare
years = ['2010', '2011','2012', '2013','2014', '2015','2016', '2017',
         '2018', '2019','2020', '2021','2022', '2023','2024']
leagues = [
    {"name": "laliga", "label": "ES1"},
    {"name": "premier-league", "label": "GB1"},
    {"name": "bundesliga", "label": "L1"},
    {"name": "serie-a", "label": "IT1"},
    {"name": "ligue-1", "label": "FR1"}
]

# Lista per i dati e URL falliti
data = []
failed_urls = []

# Configurazione retry
max_retries = 3
retry_sleep = 5

# Loop sui campionati
for league in leagues:
    league_name = league["name"]
    league_label = league["label"]

    # Loop sugli anni
    for year in years:
        url_league = f"https://www.transfermarkt.com/{league_name}/startseite/wettbewerb/{league_label}/plus/?saison_id={year}"
        squad_urls = get_squad_urls(url_league)
        squad_list = list(squad_urls)

        print(f"Processing league: {league_name}, season: {year}")
        print(squad_list)

        for url in tqdm(squad_list, desc=f"Processing {league_name} season {year}", unit=" URL"):
            url_parts = url.split("/")
            squad_id = url_parts[-3]
            squad_name = url_parts[-6].title()

            # Retry per ogni richiesta
            for attempt in range(max_retries):
                try:
                    time.sleep(randint(1, 3))  # Ritardo casuale
                    user_agent = random.choice(get_user_agents())
                    headers = {'User-Agent': user_agent, 'Referer': 'https://www.transfermarkt.com/'}

                    response = requests.get(url, headers=headers, timeout=30)
                    response.raise_for_status()

                    # Controllo per troppi tentativi (429)
                    if response.status_code == 429:
                        #print(f"Too many requests for URL: {url}. Retrying...")
                        time.sleep(retry_sleep)
                        continue

                    soup = BeautifulSoup(response.content, 'html.parser')
                    break

                except requests.exceptions.RequestException as e:
                    #print(f"Error: {e}. Retrying {attempt + 1}/{max_retries}...")
                    time.sleep(retry_sleep)

            else:
                print(f"Failed to process URL: {url} after {max_retries} attempts")
                failed_urls.append(url)
                continue

            # Estrazione dati
            try:
                table = soup.find('table', class_="items")
                if not table:
                    continue

                rows = table.find_all('tr', {'class': ['odd', 'even']})

                for row in rows:
                    shirt_number_td = row.find('td', class_="zentriert")
                    shirt_number = shirt_number_td.text.strip() if shirt_number_td else ""

                    td = row.find('td', class_="posrela")
                    if not td:
                        continue

                    td_elements = row.find_all('td', class_="zentriert")
                    td_age = td_elements[1] if len(td_elements) > 1 else None

                    td_value = row.find('td', class_="rechts hauptlink")

                    if not (td and td_value and td_age):
                        continue

                    a_tag = td.find_all('a')
                    href_value = a_tag[-1]['href']
                    href_parts = href_value.split("/")
                    name = a_tag[-1].get_text(strip=True) if len(a_tag) > 1 else a_tag[0].get_text(strip=True)

                    role = td.find('table').find_all('tr')[1].find('td').text.strip() if td.find('table') else ""

                    value = td_value.a.string if td_value.a else "0"
                    try:
                        value_num = float(value.replace('‚Ç¨', '').replace('m', '').replace('k', '').replace(',', '').strip())
                        if 'm' in value:
                            value_num *= 1_000_000
                        elif 'k' in value:
                            value_num *= 1_000
                    except ValueError:
                        value_num = 0.0

                    birth_date = td_age.get_text(strip=True) if td_age else ""

                    data.append([href_parts[-1], name, shirt_number, role, birth_date, value_num, squad_name, squad_id, year, league_name])

            except Exception as e:
                print(f"Data extraction error for URL: {url}, error: {e}")
                failed_urls.append(url)

# Mostra gli URL che non siamo riusciti a processare
if failed_urls:
    print(f"Failed URLs: {failed_urls}")
# Crea un DataFrame dai dati raccolti
df = pd.DataFrame(data, columns=['ID', 'Name', 'ShirtNumber', 'Role', 'DateOfBirth',
                                    'Value (milion of ‚Ç¨)', 'Squad', 'Squad_ID', 'Season', 'League'])
df.info()
print(df.head())
df.head()
csv_path = "/content/drive/MyDrive/PROJECT WORK/RAW_DATA/Teams_by_Season.csv"
df.to_csv(csv_path, index=False, encoding="utf-8")

#Caricamento dati su S3
import boto3
import os

# RIMOSSE le linee con AWS_ACCESS_KEY_ID e AWS_SECRET_ACCESS_KEY
# Lasciamo solo la regione, se vuoi fissarla:
AWS_REGION = "us-east-1"

# Se userai un IAM Role, puoi inizializzare cos√¨:
s3 = boto3.client(
    "s3",
    region_name=AWS_REGION
)


# üîÑ Testa la connessione elencando i tuoi bucket
buckets = s3.list_buckets()
print("‚úÖ Connessione riuscita! Ecco i tuoi bucket:")
for bucket in buckets["Buckets"]:
    print(f"- {bucket['Name']}")
# Configurazione AWS S3
S3_BUCKET_NAME = "transfermarkt-raw-data-2025"
S3_FOLDER_PATH = "Teams-by-season/"  # Percorso su S3
# üîπ Inizializza il client S3 con credenziali esplicite
s3 = boto3.client(
    "s3",
    aws_access_key_id=AWS_ACCESS_KEY_ID,
    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
    region_name=AWS_REGION
)

# üîÑ Verifica se il file esiste
if not os.path.exists(csv_path):
    print(f"‚ùå Il file {csv_path} non esiste! Assicurati di averlo salvato prima di caricarlo.")
else:
    try:
        # üîπ Carica il file su S3 (forzando l'uso delle credenziali)
        s3.upload_file(csv_path, S3_BUCKET_NAME, f"{S3_FOLDER_PATH}{os.path.basename(csv_path)}")
        print(f"‚úÖ File caricato con successo su S3: s3://{S3_BUCKET_NAME}/{S3_FOLDER_PATH}{os.path.basename(csv_path)}")
    except Exception as e:
        print(f"‚ùå Errore nel caricamento su S3: {e}")
